{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5grAcPJtUkv2NsNhNXZ53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaaboobaid/CV-Extraction/blob/main/Testing_ApplicantSystemWithDetailedFilesDownloading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31UZAnfja33v",
        "outputId": "04740526-f07d-4e1b-e916-4fb7e09f8a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Collecting phonenumbers\n",
            "  Downloading phonenumbers-9.0.21-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pdfminer.six==20251230 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-9.0.21-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.7-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab, pypdfium2, phonenumbers, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251230 pdfplumber-0.11.9 phonenumbers-9.0.21 pypdfium2-5.3.0 reportlab-4.4.7\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber spacy phonenumbers sentence-transformers pandas numpy matplotlib seaborn reportlab\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cv_full_pipeline_colab.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "import tempfile\n",
        "\n",
        "import pdfplumber\n",
        "import spacy\n",
        "import phonenumbers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"CV PROCESSING PIPELINE FOR GOOGLE COLAB\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ======================\n",
        "# Install required packages\n",
        "# ======================\n",
        "print(\"\\n[INFO] Installing required packages...\")\n",
        "!pip install pdfplumber spacy phonenumbers sentence-transformers seaborn matplotlib -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "# ======================\n",
        "# Initialize paths and directories\n",
        "# ======================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories\n",
        "BASE_DIR = \"/content/cv_processing\"\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"cv_ai_models\")\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"[INFO] Base directory: {BASE_DIR}\")\n",
        "print(f\"[INFO] Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# ======================\n",
        "# Load SpaCy model\n",
        "# ======================\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"[INFO] SpaCy model loaded successfully\")\n",
        "except:\n",
        "    print(\"[ERROR] Failed to load SpaCy model\")\n",
        "\n",
        "# ======================\n",
        "# EliteCVExtractor Class\n",
        "# ======================\n",
        "class EliteCVExtractor:\n",
        "    def __init__(self, nlp):\n",
        "        self.nlp = nlp\n",
        "        self.headers = {\n",
        "            'education': ['EDUCATION', 'ACADEMIC BACKGROUND', 'QUALIFICATIONS'],\n",
        "            'experience': ['EXPERIENCE', 'ACADEMIC EXPERIENCE', 'TEACHING EXPERIENCE',\n",
        "                           'PROFESSIONAL EXPERIENCE', 'WORK HISTORY', 'EMPLOYMENT', 'WORK EXPERIENCE'],\n",
        "            'skills': ['SKILLS', 'TECHNICAL SKILLS', 'TECHNOLOGIES', 'COMPETENCIES'],\n",
        "            'projects': ['PROJECTS', 'RESEARCH & PROJECTS', 'PUBLICATIONS', 'RESEARCH INTERESTS']\n",
        "        }\n",
        "\n",
        "    def _read_pdf(self, pdf_path):\n",
        "        lines = []\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        lines.extend(text.split('\\n'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "        return lines\n",
        "\n",
        "    def process(self, pdf_path):\n",
        "        print(f\"[INFO] Processing: {pdf_path}\")\n",
        "        lines = self._read_pdf(pdf_path)\n",
        "        full_text = \"\\n\".join(lines)\n",
        "\n",
        "        name = self._extract_name(lines)\n",
        "        phone = self._extract_phone(full_text)\n",
        "        email = self._extract_email(full_text)\n",
        "        sections = self._segment_sections(lines)\n",
        "        education_data = self._refine_education(sections.get('education', []))\n",
        "        refined_skills = self._refine_skills(sections.get('skills', []), full_text)\n",
        "        combined_experience = sections.get('experience', []) + sections.get('projects', [])\n",
        "\n",
        "        return {\n",
        "            \"personal\": {\"name\": name, \"phone\": phone, \"email\": email},\n",
        "            \"education\": education_data,\n",
        "            \"experience\": combined_experience,\n",
        "            \"skills\": refined_skills,\n",
        "            \"raw_text\": full_text\n",
        "        }\n",
        "\n",
        "    def _extract_name(self, lines):\n",
        "        for line in lines[:5]:\n",
        "            clean = line.strip()\n",
        "            if not clean: continue\n",
        "            doc = self.nlp(clean)\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ == \"PERSON\" and len(clean.split()) < 6:\n",
        "                    return ent.text.strip()\n",
        "            if clean.isupper() and len(clean.split()) < 5:\n",
        "                return clean\n",
        "        return lines[0].strip() if lines else \"Unknown\"\n",
        "\n",
        "    def _extract_phone(self, text):\n",
        "        try:\n",
        "            for match in phonenumbers.PhoneNumberMatcher(text, \"PS\"):\n",
        "                num = match.number\n",
        "                try:\n",
        "                    return phonenumbers.format_number(num, phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
        "                except:\n",
        "                    return phonenumbers.format_number(num, phonenumbers.PhoneNumberFormat.E164)\n",
        "        except:\n",
        "            pass\n",
        "        patterns = [r'\\(\\+97[02]\\)\\s?\\d{6,12}', r'\\+97[02]\\s?\\d{1,3}[\\s-]?\\d{3,6}', r'\\b05\\d{7,9}\\b', r'\\b\\+?\\d{7,15}\\b']\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text)\n",
        "            if match: return match.group(0).strip()\n",
        "        return \"Not Found\"\n",
        "\n",
        "    def _extract_email(self, text):\n",
        "        match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', text)\n",
        "        return match.group(0) if match else \"Not Found\"\n",
        "\n",
        "    def _segment_sections(self, lines):\n",
        "        sections = {k: [] for k in self.headers.keys()}\n",
        "        current_sec = None\n",
        "        for line in lines:\n",
        "            clean = line.strip()\n",
        "            if not clean: continue\n",
        "            up = clean.upper()\n",
        "            header_found = False\n",
        "            for sec, keys in self.headers.items():\n",
        "                for k in keys:\n",
        "                    if up == k or up.startswith(k):\n",
        "                        if len(up) < 60:\n",
        "                            current_sec = sec\n",
        "                            header_found = True\n",
        "                            break\n",
        "                if header_found: break\n",
        "            if header_found: continue\n",
        "            if current_sec: sections[current_sec].append(clean)\n",
        "        return sections\n",
        "\n",
        "    def _refine_education(self, edu_lines):\n",
        "        refined = []\n",
        "        for l in edu_lines:\n",
        "            if len(l.strip()) < 4: continue\n",
        "            up = l.upper()\n",
        "            if any(w in up for w in ['BACHELOR', \"BACHELOR'S\",'B.SC', 'MASTER', 'M.SC', 'PHD', 'PH.D', 'DOCTOR']):\n",
        "                refined.append(l.strip())\n",
        "            elif any(x in up for x in ['UNIVERSITY', 'COLLEGE', 'INSTITUTE', 'SCHOOL', 'FACULTY']):\n",
        "                refined.append(l.strip())\n",
        "            elif re.search(r'\\b(19|20)\\d{2}\\b', l):\n",
        "                refined.append(l.strip())\n",
        "            elif len(l.strip()) > 30:\n",
        "                refined.append(l.strip())\n",
        "        return refined\n",
        "\n",
        "    def _refine_skills(self, skill_lines, full_text):\n",
        "        text = \" \".join(skill_lines) if skill_lines and sum(len(s) for s in skill_lines) > 20 else full_text\n",
        "        doc = self.nlp(text)\n",
        "        skills = set()\n",
        "        for token in doc:\n",
        "            if token.pos_ in ['PROPN', 'NOUN'] and len(token.text) > 2:\n",
        "                cleaned = re.sub(r'[^A-Za-z0-9\\+\\#\\.\\-]', '', token.text)\n",
        "                if len(cleaned) > 1: skills.add(cleaned)\n",
        "        whitelist = ['C++', 'C#', 'Go', 'React', 'Vue', 'Node.js', 'Latex', 'Docker', 'Kubernetes',\n",
        "                     'SQL', 'NoSQL', 'Git', 'Linux', 'Matlab', 'Simulink', 'IoT', 'Python', 'Java',\n",
        "                     'TensorFlow', 'PyTorch', 'OpenCV', 'Spark', 'AWS', 'Azure', 'MongoDB', 'Flutter',\n",
        "                     'React Native']\n",
        "        for w in whitelist:\n",
        "            if re.search(r'\\b' + re.escape(w) + r'\\b', text, flags=re.IGNORECASE):\n",
        "                skills.add(w)\n",
        "        cleaned_skills = [s.strip() for s in skills if len(s.strip()) > 1]\n",
        "        return sorted(set(cleaned_skills), key=lambda x: x.lower())\n",
        "\n",
        "# ======================\n",
        "# Internship Matcher Class\n",
        "# ======================\n",
        "class InternshipMatcher:\n",
        "    def __init__(self, model, df):\n",
        "        self.model = model\n",
        "        self.df = df.copy()\n",
        "        self.df[\"combined_text\"] = (\n",
        "            self.df[\"position\"] + \" \" +\n",
        "            self.df[\"required_skills\"] + \" \" +\n",
        "            self.df[\"description\"]\n",
        "        )\n",
        "        print(\"[INFO] Encoding internship embeddings...\")\n",
        "        self.job_embeddings = self.model.encode(self.df[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "    def calculate_ats_score(self, cv):\n",
        "        breakdown = {}\n",
        "        contact = 0\n",
        "        if cv[\"personal\"][\"name\"] != \"Unknown\": contact += 5\n",
        "        if cv[\"personal\"][\"phone\"] != \"Not Found\": contact += 5\n",
        "        if cv[\"personal\"][\"email\"] != \"Not Found\": contact += 5\n",
        "        breakdown[\"Contact\"] = contact\n",
        "\n",
        "        edu_text = \" \".join(cv[\"education\"]).upper()\n",
        "        edu_score = 10\n",
        "        if \"PHD\" in edu_text or \"DOCTOR\" in edu_text: edu_score += 25\n",
        "        elif \"MASTER\" in edu_text or \"M.SC\" in edu_text: edu_score += 15\n",
        "        elif \"BACHELOR\" in edu_text or \"B.SC\" in edu_text: edu_score += 10\n",
        "        breakdown[\"Education\"] = min(35, edu_score)\n",
        "\n",
        "        exp_text = \" \".join(cv[\"experience\"]).upper()\n",
        "        exp_score = 10\n",
        "        if any(k in exp_text for k in [\"PROFESSOR\", \"LECTURER\", \"MANAGER\", \"LEAD\", \"CHAIR\"]):\n",
        "            exp_score += 20\n",
        "        elif len(cv[\"experience\"]) > 15: exp_score += 15\n",
        "        elif len(cv[\"experience\"]) > 5: exp_score += 5\n",
        "        breakdown[\"Experience\"] = min(30, exp_score)\n",
        "\n",
        "        breakdown[\"Skills\"] = min(20, len(cv[\"skills\"]) + 5)\n",
        "        total = sum(breakdown.values())\n",
        "        return total, breakdown\n",
        "\n",
        "    def match(self, cv, ats_score):\n",
        "        candidate_text = \" \".join(cv[\"skills\"]) + \" \" + \" \".join(cv[\"experience\"]) + \" \" + \" \".join(cv[\"education\"])\n",
        "        cand_embedding = self.model.encode(candidate_text, convert_to_tensor=True)\n",
        "        similarities = util.cos_sim(cand_embedding, self.job_embeddings)[0]\n",
        "\n",
        "        results = []\n",
        "        for idx, sim in enumerate(similarities):\n",
        "            semantic_pct = float(sim) * 100\n",
        "            final_score = 0.6 * semantic_pct + 0.4 * ats_score\n",
        "\n",
        "            # Handle stipend safely\n",
        "            try:\n",
        "                stipend = float(self.df.iloc[idx][\"monthly_stipend\"])\n",
        "            except:\n",
        "                stipend = 0.0\n",
        "\n",
        "            results.append({\n",
        "                \"company\": str(self.df.iloc[idx][\"company\"]),\n",
        "                \"position\": str(self.df.iloc[idx][\"position\"]),\n",
        "                \"location\": str(self.df.iloc[idx][\"location\"]),\n",
        "                \"stipend\": stipend,\n",
        "                \"semantic_score\": round(semantic_pct, 2),\n",
        "                \"final_score\": round(final_score, 2),\n",
        "                \"required_skills\": str(self.df.iloc[idx][\"required_skills\"]),\n",
        "                \"description\": str(self.df.iloc[idx][\"description\"])\n",
        "            })\n",
        "\n",
        "        return sorted(results, key=lambda x: x[\"final_score\"], reverse=True)\n",
        "\n",
        "# ======================\n",
        "# Dashboard Creation Function\n",
        "# ======================\n",
        "def create_dashboard(cv_data, ats_breakdown, matches, output_path):\n",
        "    \"\"\"Create dashboard visualization\"\"\"\n",
        "    try:\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 14))\n",
        "        fig.suptitle(\n",
        "            f\"CV ATS & Internship Dashboard\\n{cv_data['personal']['name']}\",\n",
        "            fontsize=22,\n",
        "            fontweight=\"bold\",\n",
        "            y=0.97\n",
        "        )\n",
        "\n",
        "        # 1. ATS DONUT\n",
        "        ax1 = fig.add_subplot(231)\n",
        "        ax1.pie(ats_breakdown.values(), labels=ats_breakdown.keys(), autopct=\"%1.1f%%\", startangle=140)\n",
        "        ax1.add_artist(plt.Circle((0,0),0.7,fc=\"white\"))\n",
        "        ax1.text(0,0,str(sum(ats_breakdown.values())),ha=\"center\",va=\"center\",fontsize=26,fontweight=\"bold\")\n",
        "        ax1.set_title(\"ATS Score Breakdown\", fontweight=\"bold\")\n",
        "\n",
        "        # 2. RADAR\n",
        "        ax2 = fig.add_subplot(232, polar=True)\n",
        "        categories = list(ats_breakdown.keys())\n",
        "        values = list(ats_breakdown.values())\n",
        "        max_values = [15,35,30,20]\n",
        "        values_norm = [(v/m)*100 for v,m in zip(values,max_values)]\n",
        "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
        "        angles = np.concatenate((angles, [angles[0]]))\n",
        "        values_norm = np.concatenate((values_norm, [values_norm[0]]))\n",
        "\n",
        "        ax2.plot(angles, values_norm, linewidth=2)\n",
        "        ax2.fill(angles, values_norm, alpha=0.3)\n",
        "        ax2.set_ylim(0,120)\n",
        "        ax2.set_xticks([])\n",
        "        ax2.set_title(\"Profile Strength\", pad=35, fontweight=\"bold\")\n",
        "\n",
        "        # 3. Top skills\n",
        "        ax3 = fig.add_subplot(233)\n",
        "        top_skills = cv_data[\"skills\"][:10]\n",
        "        if top_skills:\n",
        "            sns.barplot(x=list(range(len(top_skills))), y=top_skills, ax=ax3)\n",
        "        ax3.set_title(\"Top Skills Detected\", fontweight=\"bold\")\n",
        "        ax3.set_xlabel(\"Relevance\")\n",
        "        ax3.set_ylabel(\"Skill\")\n",
        "\n",
        "        # 4. Top 5 Matches\n",
        "        ax4 = fig.add_subplot(234)\n",
        "        top5 = matches[:5]\n",
        "        if top5:\n",
        "            labels = [f\"{m['company']}\\n{m['position']}\" for m in top5]\n",
        "            scores = [m[\"final_score\"] for m in top5]\n",
        "            sns.barplot(x=scores, y=labels, ax=ax4)\n",
        "            ax4.set_xlim(0,100)\n",
        "        ax4.set_title(\"Top 5 Internship Matches\", fontweight=\"bold\")\n",
        "        ax4.set_xlabel(\"Fit Score (%)\")\n",
        "\n",
        "        # 5. Stipend vs Fit\n",
        "        ax5 = fig.add_subplot(235)\n",
        "        top10 = matches[:10]\n",
        "        if top10:\n",
        "            sns.scatterplot(x=[m[\"stipend\"] for m in top10], y=[m[\"final_score\"] for m in top10], s=300, ax=ax5)\n",
        "        ax5.set_xlabel(\"Monthly Stipend\")\n",
        "        ax5.set_ylabel(\"Fit Score\")\n",
        "        ax5.set_title(\"Stipend vs Fit\", fontweight=\"bold\")\n",
        "\n",
        "        # 6. Locations\n",
        "        ax6 = fig.add_subplot(236)\n",
        "        locations = [m[\"location\"] for m in matches[:10]]\n",
        "        loc_counts = Counter(locations)\n",
        "        if loc_counts:\n",
        "            ax6.pie(loc_counts.values(), labels=loc_counts.keys(), autopct=\"%1.1f%%\", startangle=140)\n",
        "        ax6.set_title(\"Top Locations\", fontweight=\"bold\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"[INFO] Dashboard saved: {output_path}\")\n",
        "\n",
        "        # Display the dashboard in Colab\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        img = plt.imread(output_path)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(\"CV Analysis Dashboard\", fontsize=16, fontweight='bold')\n",
        "        plt.show()\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to create dashboard: {e}\")\n",
        "        return False\n",
        "\n",
        "# ======================\n",
        "# Functions to save JSON files with specific format\n",
        "# ======================\n",
        "def save_education_json(education_data, output_path):\n",
        "    \"\"\"Save education data in JSON format with commas between degrees\"\"\"\n",
        "    try:\n",
        "        # Prepare education data as a list\n",
        "        education_list = education_data if isinstance(education_data, list) else []\n",
        "\n",
        "        # Save as JSON with custom separators\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(education_list, f, ensure_ascii=False, separators=(',', ':'))\n",
        "        print(f\"[INFO] Education JSON saved: {output_path}\")\n",
        "\n",
        "        # Display the content\n",
        "        print(\"\\nEducation Data (JSON format):\")\n",
        "        print(\"-\" * 50)\n",
        "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read())\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to save education JSON: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_experience_json(experience_data, output_path):\n",
        "    \"\"\"Save experience data in JSON format with commas between experiences\"\"\"\n",
        "    try:\n",
        "        # Prepare experience data as a list\n",
        "        experience_list = experience_data if isinstance(experience_data, list) else []\n",
        "\n",
        "        # Save as JSON with custom separators\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(experience_list, f, ensure_ascii=False, separators=(',', ':'))\n",
        "        print(f\"[INFO] Experience JSON saved: {output_path}\")\n",
        "\n",
        "        # Display the content\n",
        "        print(\"\\nExperience Data (JSON format):\")\n",
        "        print(\"-\" * 50)\n",
        "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read())\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to save experience JSON: {e}\")\n",
        "        return False\n",
        "\n",
        "# ======================\n",
        "# Main Pipeline Function\n",
        "# ======================\n",
        "def run_pipeline(cv_file_path, csv_file_path=None):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STARTING CV PROCESSING PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # ===== Extraction =====\n",
        "        extractor = EliteCVExtractor(nlp)\n",
        "        cv_data = extractor.process(cv_file_path)\n",
        "\n",
        "        # Display extracted information\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTED CV INFORMATION\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Name: {cv_data['personal']['name']}\")\n",
        "        print(f\"Email: {cv_data['personal']['email']}\")\n",
        "        print(f\"Phone: {cv_data['personal']['phone']}\")\n",
        "        print(f\"Education items: {len(cv_data['education'])}\")\n",
        "        print(f\"Experience items: {len(cv_data['experience'])}\")\n",
        "        print(f\"Skills found: {len(cv_data['skills'])}\")\n",
        "\n",
        "        # Save extracted data\n",
        "        base_name = os.path.basename(cv_file_path).replace(\".pdf\", \"\")\n",
        "\n",
        "        # Save education in requested JSON format\n",
        "        education_json_path = os.path.join(OUTPUT_DIR, f\"{base_name}_education.json\")\n",
        "        save_education_json(cv_data[\"education\"], education_json_path)\n",
        "\n",
        "        # Save experience in requested JSON format\n",
        "        experience_json_path = os.path.join(OUTPUT_DIR, f\"{base_name}_experience.json\")\n",
        "        save_experience_json(cv_data[\"experience\"], experience_json_path)\n",
        "\n",
        "        # Save full extracted data\n",
        "        out_json = os.path.join(OUTPUT_DIR, f\"{base_name}_extracted.json\")\n",
        "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(cv_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"\\n[INFO] Full extracted data saved: {out_json}\")\n",
        "\n",
        "        # Display skills\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DETECTED SKILLS\")\n",
        "        print(\"=\"*50)\n",
        "        for i, skill in enumerate(cv_data[\"skills\"][:20], 1):\n",
        "            print(f\"{i}. {skill}\")\n",
        "        if len(cv_data[\"skills\"]) > 20:\n",
        "            print(f\"... and {len(cv_data['skills']) - 20} more\")\n",
        "\n",
        "        # ===== Check if matching should be done =====\n",
        "        if csv_file_path and os.path.exists(csv_file_path):\n",
        "            print(f\"\\n[INFO] Loading internship data from: {csv_file_path}\")\n",
        "            df = pd.read_csv(csv_file_path).fillna(\"\")\n",
        "            print(f\"[INFO] Loaded {len(df)} internships\")\n",
        "\n",
        "            # Load SBERT model\n",
        "            model_path = os.path.join(MODEL_DIR, \"sbert_model\")\n",
        "            if not os.path.exists(model_path):\n",
        "                print(\"[INFO] Downloading SBERT model...\")\n",
        "                sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "                sbert.save(model_path)\n",
        "            else:\n",
        "                sbert = SentenceTransformer(model_path)\n",
        "            print(\"[INFO] SBERT model ready\")\n",
        "\n",
        "            # ===== Matching =====\n",
        "            matcher = InternshipMatcher(sbert, df)\n",
        "            ats_score, ats_breakdown = matcher.calculate_ats_score(cv_data)\n",
        "            matches = matcher.match(cv_data, ats_score)\n",
        "\n",
        "            # Display ATS score\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"ATS SCORE ANALYSIS\")\n",
        "            print(\"=\"*50)\n",
        "            print(f\"Overall ATS Score: {ats_score}/100\")\n",
        "            for category, score in ats_breakdown.items():\n",
        "                print(f\"  {category}: {score}\")\n",
        "\n",
        "            # Save matching results\n",
        "            matches_json = os.path.join(OUTPUT_DIR, f\"{base_name}_matches.json\")\n",
        "            matching_results = {\n",
        "                \"ats_score\": ats_score,\n",
        "                \"ats_breakdown\": ats_breakdown,\n",
        "                \"total_matches\": len(matches),\n",
        "                \"matches\": matches[:50],\n",
        "                \"top_matches\": matches[:10]\n",
        "            }\n",
        "\n",
        "            with open(matches_json, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(matching_results, f, indent=4, ensure_ascii=False)\n",
        "            print(f\"[INFO] Matching results saved: {matches_json}\")\n",
        "\n",
        "            # Display top matches\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"TOP 10 INTERNSHIP MATCHES\")\n",
        "            print(\"=\"*50)\n",
        "            for i, match in enumerate(matches[:10], 1):\n",
        "                print(f\"{i}. {match['company']} - {match['position']}\")\n",
        "                print(f\"   Fit Score: {match['final_score']:.1f}% | Stipend: ${match['stipend']:,.0f}\")\n",
        "                print(f\"   Location: {match['location']}\")\n",
        "                print()\n",
        "\n",
        "            # ===== Generate Dashboard =====\n",
        "            dashboard_path = os.path.join(OUTPUT_DIR, f\"{base_name}_dashboard.png\")\n",
        "            print(f\"\\n[INFO] Generating dashboard...\")\n",
        "            create_dashboard(cv_data, ats_breakdown, matches, dashboard_path)\n",
        "\n",
        "            # ===== Save summary =====\n",
        "            summary_json = os.path.join(OUTPUT_DIR, f\"{base_name}_summary.json\")\n",
        "            summary = {\n",
        "                \"applicant_name\": cv_data[\"personal\"][\"name\"],\n",
        "                \"applicant_email\": cv_data[\"personal\"][\"email\"],\n",
        "                \"applicant_phone\": cv_data[\"personal\"][\"phone\"],\n",
        "                \"ats_score\": ats_score,\n",
        "                \"total_matches\": len(matches),\n",
        "                \"top_3_matches\": [\n",
        "                    {\n",
        "                        \"company\": match[\"company\"],\n",
        "                        \"position\": match[\"position\"],\n",
        "                        \"score\": match[\"final_score\"],\n",
        "                        \"stipend\": match[\"stipend\"]\n",
        "                    }\n",
        "                    for match in matches[:3]\n",
        "                ],\n",
        "                \"files\": {\n",
        "                    \"extracted\": os.path.basename(out_json),\n",
        "                    \"education\": os.path.basename(education_json_path),\n",
        "                    \"experience\": os.path.basename(experience_json_path),\n",
        "                    \"matches\": os.path.basename(matches_json),\n",
        "                    \"dashboard\": os.path.basename(dashboard_path)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(summary, f, indent=4, ensure_ascii=False)\n",
        "            print(f\"[INFO] Summary saved: {summary_json}\")\n",
        "\n",
        "            # ===== Prepare download links =====\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"GENERATED FILES\")\n",
        "            print(\"=\"*50)\n",
        "            files_to_download = [\n",
        "                out_json,\n",
        "                education_json_path,\n",
        "                experience_json_path,\n",
        "                matches_json,\n",
        "                dashboard_path,\n",
        "                summary_json\n",
        "            ]\n",
        "\n",
        "            for file_path in files_to_download:\n",
        "                if os.path.exists(file_path):\n",
        "                    print(f\"✓ {os.path.basename(file_path)}\")\n",
        "\n",
        "            # Ask user if they want to download files\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"DOWNLOAD OPTIONS\")\n",
        "            print(\"=\"*50)\n",
        "            print(\"1. Download all generated files\")\n",
        "            print(\"2. Download specific files\")\n",
        "            print(\"3. Continue without downloading\")\n",
        "\n",
        "            choice = input(\"\\nEnter your choice (1-3): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                # Create a zip file of all outputs\n",
        "                import zipfile\n",
        "                zip_path = os.path.join(OUTPUT_DIR, f\"{base_name}_results.zip\")\n",
        "                with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "                    for file_path in files_to_download:\n",
        "                        if os.path.exists(file_path):\n",
        "                            zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "                print(f\"\\n[INFO] Zip file created: {zip_path}\")\n",
        "                files.download(zip_path)\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                print(\"\\nAvailable files:\")\n",
        "                for i, file_path in enumerate(files_to_download, 1):\n",
        "                    if os.path.exists(file_path):\n",
        "                        print(f\"{i}. {os.path.basename(file_path)}\")\n",
        "\n",
        "                file_choices = input(\"\\nEnter file numbers to download (comma-separated): \").strip()\n",
        "                try:\n",
        "                    indices = [int(idx.strip()) - 1 for idx in file_choices.split(\",\")]\n",
        "                    for idx in indices:\n",
        "                        if 0 <= idx < len(files_to_download):\n",
        "                            file_path = files_to_download[idx]\n",
        "                            if os.path.exists(file_path):\n",
        "                                files.download(file_path)\n",
        "                except:\n",
        "                    print(\"[ERROR] Invalid input\")\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"matching\": True,\n",
        "                \"applicant\": cv_data[\"personal\"],\n",
        "                \"ats_score\": ats_score,\n",
        "                \"total_matches\": len(matches),\n",
        "                \"top_matches\": matches[:5],\n",
        "                \"files\": files_to_download\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(\"\\n[INFO] No internship CSV provided or file not found.\")\n",
        "            print(\"[INFO] Skipping matching process.\")\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"matching\": False,\n",
        "                \"applicant\": cv_data[\"personal\"],\n",
        "                \"files\": [out_json, education_json_path, experience_json_path]\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# ======================\n",
        "# Main execution for Colab\n",
        "# ======================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CV PROCESSING - INTERACTIVE MODE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ask user for input method\n",
        "    print(\"\\nChoose input method:\")\n",
        "    print(\"1. Upload PDF file\")\n",
        "    print(\"2. Use sample CV from Drive\")\n",
        "    print(\"3. Enter path to PDF file\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-3): \").strip()\n",
        "\n",
        "    cv_file_path = None\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Upload PDF file\n",
        "        print(\"\\nPlease upload your CV (PDF file):\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            file_name = list(uploaded.keys())[0]\n",
        "            cv_file_path = os.path.join(\"/content\", file_name)\n",
        "            print(f\"[INFO] File uploaded: {cv_file_path}\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # Use file from Google Drive\n",
        "        print(\"\\nEnter the path to your CV in Google Drive\")\n",
        "        print(\"Example: /content/drive/MyDrive/CVs/my_cv.pdf\")\n",
        "        drive_path = input(\"Path: \").strip()\n",
        "        if os.path.exists(drive_path):\n",
        "            cv_file_path = drive_path\n",
        "            print(f\"[INFO] Using file: {cv_file_path}\")\n",
        "        else:\n",
        "            print(f\"[ERROR] File not found: {drive_path}\")\n",
        "            return\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Enter file path\n",
        "        file_path = input(\"\\nEnter the full path to your PDF file: \").strip()\n",
        "        if os.path.exists(file_path):\n",
        "            cv_file_path = file_path\n",
        "            print(f\"[INFO] Using file: {cv_file_path}\")\n",
        "        else:\n",
        "            print(f\"[ERROR] File not found: {file_path}\")\n",
        "            return\n",
        "\n",
        "    else:\n",
        "        print(\"[ERROR] Invalid choice\")\n",
        "        return\n",
        "\n",
        "    # Ask for internship CSV\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"INTERNSHIP MATCHING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nChoose internship data source:\")\n",
        "    print(\"1. Upload CSV file\")\n",
        "    print(\"2. Use sample CSV from Drive\")\n",
        "    print(\"3. Skip matching (extract only)\")\n",
        "\n",
        "    csv_choice = input(\"\\nEnter your choice (1-3): \").strip()\n",
        "\n",
        "    csv_file_path = None\n",
        "\n",
        "    if csv_choice == \"1\":\n",
        "        # Upload CSV file\n",
        "        print(\"\\nPlease upload your internship CSV file:\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            csv_file_name = list(uploaded.keys())[0]\n",
        "            csv_file_path = os.path.join(\"/content\", csv_file_name)\n",
        "            print(f\"[INFO] CSV uploaded: {csv_file_path}\")\n",
        "\n",
        "    elif csv_choice == \"2\":\n",
        "        # Use CSV from Google Drive\n",
        "        print(\"\\nEnter the path to your internship CSV in Google Drive\")\n",
        "        print(\"Example: /content/palestinian_internships_200.csv\")\n",
        "        csv_drive_path = input(\"Path: \").strip()\n",
        "        if os.path.exists(csv_drive_path):\n",
        "            csv_file_path = csv_drive_path\n",
        "            print(f\"[INFO] Using CSV: {csv_file_path}\")\n",
        "        else:\n",
        "            print(f\"[WARNING] CSV file not found: {csv_drive_path}\")\n",
        "            print(\"[INFO] Continuing without matching...\")\n",
        "\n",
        "    elif csv_choice == \"3\":\n",
        "        print(\"[INFO] Skipping matching process...\")\n",
        "\n",
        "    else:\n",
        "        print(\"[ERROR] Invalid choice\")\n",
        "        return\n",
        "\n",
        "    # Run the pipeline\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STARTING PROCESSING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    result = run_pipeline(cv_file_path, csv_file_path)\n",
        "\n",
        "    # Display final summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if result[\"success\"]:\n",
        "        if result.get(\"matching\", False):\n",
        "            print(f\"✓ CV processed successfully!\")\n",
        "            print(f\"✓ Applicant: {result['applicant']['name']}\")\n",
        "            print(f\"✓ ATS Score: {result['ats_score']}/100\")\n",
        "            print(f\"✓ Matches found: {result['total_matches']}\")\n",
        "        else:\n",
        "            print(f\"✓ CV extracted successfully!\")\n",
        "            print(f\"✓ Applicant: {result['applicant']['name']}\")\n",
        "            print(f\"✓ Education data saved in JSON format\")\n",
        "            print(f\"✓ Experience data saved in JSON format\")\n",
        "    else:\n",
        "        print(f\"✗ Processing failed: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "# ======================\n",
        "# Run the main function\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M5-_FHvCfn9Q",
        "outputId": "f045eaa6-9d7e-45b9-ccc5-d0c5414c9d36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "CV PROCESSING PIPELINE FOR GOOGLE COLAB\n",
            "==================================================\n",
            "\n",
            "[INFO] Installing required packages...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Mounted at /content/drive\n",
            "[INFO] Base directory: /content/cv_processing\n",
            "[INFO] Output directory: /content/cv_processing/output\n",
            "[INFO] SpaCy model loaded successfully\n",
            "\n",
            "==================================================\n",
            "CV PROCESSING - INTERACTIVE MODE\n",
            "==================================================\n",
            "\n",
            "Choose input method:\n",
            "1. Upload PDF file\n",
            "2. Use sample CV from Drive\n",
            "3. Enter path to PDF file\n",
            "\n",
            "Enter your choice (1-3): 1\n",
            "\n",
            "Please upload your CV (PDF file):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8c76f3e-499a-4811-8fe6-a662e05128f9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e8c76f3e-499a-4811-8fe6-a662e05128f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Anas_CV.pdf to Anas_CV.pdf\n",
            "[INFO] File uploaded: /content/Anas_CV.pdf\n",
            "\n",
            "==================================================\n",
            "INTERNSHIP MATCHING\n",
            "==================================================\n",
            "\n",
            "Choose internship data source:\n",
            "1. Upload CSV file\n",
            "2. Use sample CSV from Drive\n",
            "3. Skip matching (extract only)\n",
            "\n",
            "Enter your choice (1-3): 3\n",
            "[INFO] Skipping matching process...\n",
            "\n",
            "==================================================\n",
            "STARTING PROCESSING\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "STARTING CV PROCESSING PIPELINE\n",
            "==================================================\n",
            "[INFO] Processing: /content/Anas_CV.pdf\n",
            "\n",
            "==================================================\n",
            "EXTRACTED CV INFORMATION\n",
            "==================================================\n",
            "Name: ANAS MELHEM\n",
            "Email: a.melhem@ptuk.edu.ps\n",
            "Phone: +970 599 320 207\n",
            "Education items: 10\n",
            "Experience items: 73\n",
            "Skills found: 20\n",
            "[INFO] Education JSON saved: /content/cv_processing/output/Anas_CV_education.json\n",
            "\n",
            "Education Data (JSON format):\n",
            "--------------------------------------------------\n",
            "[\"• PhD in Computer Engineering\",\"Eastern Mediterranean University, North Cyprus. September, 2021\",\"– Thesis title: Analysis and Development of Ciphers Homomorphic on Addition and\",\"– Advisor: Prof. Dr. Alexander Chefranov.\",\"• Master’s in Electronics and Computer Engineering\",\"Al-Quds University, Palestine. May, 2012\",\"– Thesis title: Ticket Authentication Wireless Mesh Networks Protocol.\",\"– Advisor: Assoc. Prof. Dr. Rushdi Hamamreh.\",\"• Bachelor’s in Electrical Engineering\",\"Palestine Technical University, Palestine. February, 2005\"]\n",
            "[INFO] Experience JSON saved: /content/cv_processing/output/Anas_CV_experience.json\n",
            "\n",
            "Experience Data (JSON format):\n",
            "--------------------------------------------------\n",
            "[\"Assistant Professor\",\"Computer Systems Engineering Dept., Palestine Technical University 2021 – Present\",\"• Conducting pioneering research in post-quantum security.\",\"• teaching courses in Cryptography and Network Security, Digital Logic Design, Data Mining, Dis-\",\"crete Mathematics, Operating Systems, and Computer Networks.\",\"• Participating in multiple committees for revising course descriptions and preparing proposals for\",\"accrediting new programs.\",\"Lecturer\",\"Computer Systems Engineering Dept., Palestine Technical University 2016 – 2021\",\"• Taught various courses in Computer Systems Engineering Department.\",\"• Participated in the Erasmus+ project titled “Pathway in Forensic Computing”.\",\"Research Assistant\",\"Computer Engineering Dept., Eastern Mediterranean University, North Cyprus 2016 – 2018\",\"• Researched homomorphic cryptosystems.\",\"• Taught several labs, including Operating Systems, Introduction to Programming, and Database\",\"Systems.\",\"• Participated in a workgroup focused on preparing for ABET accreditation.\",\"Lab Engineer\",\"Computer Systems Engineering Dept., Palestine Technical University, 2009 – 2013\",\"• Instructing multiple labs including Digital Logic Design, Computer Architecture, Computer Net-\",\"works, and Operating Systems.\",\"My journey in academia began in 2016 when I was promoted to lecturer in the Computer Systems\",\"Engineering Department at Palestine Technical University. Since then, I have instructed multiple\",\"undergraduate courses, including:\",\"• 12140527 Cryptography and Network Security\",\"• 12140420 Digital Logic Design\",\"• 12140204 Discrete Mathematics\",\"• 12140535 Data Mining\",\"• 12140312 Computer Networks\",\"• 12140308 Operating Systems\",\"Erasmus+ Project: Pathway in Forensic Computing\",\"• Authoredthechapter“DigitalForensicsEvidenceAcquisition”inthebookDigitalInvestigation\",\"Techniques and Tools.\",\"• Participated in technical workshops in Palestine and Jordan.\",\"PhD Research Projects\",\"• Development of Post-Quantum Cryptosystem: DevelopedRCPKC,anovelpublickeycryp-\",\"tosystem that is immune to lattice-based attacks and significantly faster than NTRU. It is partic-\",\"ularly suitable for power-constrained devices.\",\"• RSA Security Analysis: Developed a ciphertext-only attack using lattice basis reduction, ef-\",\"fective against keys up to 8193 bits.\",\"• NTRU Cryptosystem Analysis: Designed the NTRU modulo p flaw attack, with recommen-\",\"dations for parameter settings to mitigate the attack.\",\"• HE1N Cryptosystem Analysis: Developed Known Plaintext Attacks (KPA) and ciphertext-\",\"only attacks (COA) against the HE1N cryptosystem, with new parameter settings to mitigate\",\"these attacks.\",\"Journal Papers\",\"· Anas Ibrahim, Alexander Chefranov, Rushdi Hamamreh,”Ciphertext-Only Attack on RSA Using Lat-\",\"tice Basis Reduction”, The International Arab Journal of Information Technology, vol. 18, no. 2, pp.\",\"237 – 247, March. 2021.\",\"· AnasIbrahim,AlexanderChefranov,NaghamHamad,Yousef-AwwadDaraghmi,AhmadAl-Khasawneh,\",\"JoelJ.P.C.Rodrigues,”NTRU-LikeRandomCongruentialPublic-KeyCryptosystemforWirelessSen-\",\"sor Networks”, Sensors, vol. 20, no. 16, pp. 4632 – 4657, Aug. 2020.\",\"· Chuck Easttom, Anas Ibrahim, Alexander Chefranov, Izzat Alsmadi, Richard Hansen,”Towards A\",\"Deeper NTRU Analysis: A Multi Modal Analysis”, International Journal on Cryptography and Infor-\",\"mation Security (IJCIS), vol. 10, no. 2, pp. 11 – 22, Jun. 2020.\",\"· Anas Ibrahim, Alexander Chefranov,”NTRU Modulo p Flaw”, International Journal for Information\",\"Security Research (IJISR), vol. 6, no. 3, pp. 685 – 690, Sep. 2016.\",\"· Rushdi Hamamreh, Anas Melhem, ”SWMPT: Securing Wireless Mesh Networks Protocol Based on\",\"Ticket Authentication”, The Research Bulletin of Jordan ACM, vol. 2, no. 4, pp. 129 – 133, 2011.\",\"· Rushdi Hamamreh, Anas Melhem, ”Securing End-to-End Wireless Mesh Networks Ticket Based\",\"Authentication”, GSTF Journal on Computing (JoC), vol. 1, no. 2, 2011.\",\"Conference Papers\",\"· Anas Ibrahim, Alexander Chefranov, Nagham Hamad, ”NTRU-Like Secure and Effective Congruential\",\"Public-Key Cryptosystem Using Big Numbers”, in Proc. 2019 2nd International Conference on new\",\"Trends in Computing Sciences (ICTCS), Amman, Jordan, 9-11 Oct. 2019.\",\"· Alexander Chefranov, Anas Ibrahim, ”NTRU Modulo p Flaw”, in Proc. World Congress on Internet\",\"Security, WorldCIS 2016, London, UK, November 14-16, 2016.\",\"· Rushdi Hamamreh, Anas Melhem, ”Secure Mobile Clients Using Elliptic Curve for WMN”. in Proc.\",\"The 13th International Arab Conference on Information Technology, ACIT 2012, Zarqa, Jordan,\",\"December 10-13, 2012.\",\"· Rushdi Hamamreh, Anas Melhem, ”THWMP: A Ticket-Based Secure Hybrid Wireless Mesh Networks\",\"Protocol”, in Proc. 2011 Conference on Innovations in Computing and Engineering Machinery,\",\"CICEM2011, Amman, Jordan, September 5-7, 2011.\"]\n",
            "\n",
            "[INFO] Full extracted data saved: /content/cv_processing/output/Anas_CV_extracted.json\n",
            "\n",
            "==================================================\n",
            "DETECTED SKILLS\n",
            "==================================================\n",
            "1. Algorithm\n",
            "2. computing\n",
            "3. Cryptanalysis\n",
            "4. Cryptographic\n",
            "5. curriculum\n",
            "6. Curriculum\n",
            "7. Design\n",
            "8. DEVELOPMENT\n",
            "9. Development\n",
            "10. development\n",
            "11. Erasmus+\n",
            "12. MemberofthecommitteefordevelopingtheMastersprograminSoftwareEngineeringatPalestine\n",
            "13. MEMBERSHIPS\n",
            "14. Mentoring\n",
            "15. PROFESSIONAL\n",
            "16. projects\n",
            "17. Teaching\n",
            "18. Technical\n",
            "19. University\n",
            "20. workshops\n",
            "\n",
            "[INFO] No internship CSV provided or file not found.\n",
            "[INFO] Skipping matching process.\n",
            "\n",
            "==================================================\n",
            "PROCESSING COMPLETE\n",
            "==================================================\n",
            "✓ CV extracted successfully!\n",
            "✓ Applicant: ANAS MELHEM\n",
            "✓ Education data saved in JSON format\n",
            "✓ Experience data saved in JSON format\n"
          ]
        }
      ]
    }
  ]
}